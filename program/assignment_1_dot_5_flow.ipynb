{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "20d212ff", "cell_type": "markdown", "source": "# Save to S3 with a SageMaker Processing Job\n\n<div class=\"alert alert-info\"> \ud83d\udca1 <strong> Quick Start </strong>\nTo save your processed data to S3, select the Run menu above and click <strong>Run all cells</strong>. \n<strong><a style=\"color: #0397a7 \" href=\"#Job-Status-&-S3-Output-Location\">\n    <u>View the status of the export job and the output S3 location</u></a>.\n</strong>\n</div>\n\n\nThis notebook executes your data flow `penguins.flow` on the entire dataset using a SageMaker \nProcessing Job and will save the processed data to S3.\n\nThis notebook saves data from the step `Encode Categorical` from `Source: Data.Csv`. To save from a different step, go to Canvas \nto select a new step to export. \n\n---\n\n## Contents\n\n1. [Inputs and Outputs](#Inputs-and-Outputs)\n1. [Run Processing Job](#Run-Processing-Job)\n   1. [Job Configurations](#Job-Configurations)\n   1. [Create Processing Job](#Create-Processing-Job)\n   1. [Job Status & S3 Output Location](#Job-Status-&-S3-Output-Location)\n1. [Optional Next Steps](#(Optional)Next-Steps)\n    1. [Load Processed Data into Pandas](#(Optional)-Load-Processed-Data-into-Pandas)\n    1. [Train a model with SageMaker](#(Optional)Train-a-model-with-SageMaker)\n---", "metadata": {}}, {"id": "e8380947", "cell_type": "markdown", "source": "# Inputs and Outputs\n\nThe below settings configure the inputs and outputs for the flow export.\n\n<div class=\"alert alert-info\"> \ud83d\udca1 <strong> Configurable Settings </strong>\n\nIn <b>Input - Source</b> you can configure the data sources that will be used as input by Canvas\n\n1. For S3 sources, configure the source attribute that points to the input S3 prefixes\n2. For all other sources, configure attributes like query_string, database in the source's \n<b>DatasetDefinition</b> object.\n\nIf you modify the inputs the provided data must have the same schema and format as the data used in the Flow. \nYou should also re-execute the cells in this section if you have modified the settings in any data sources.\n\nParametrized data sources will be ignored when creating ProcessingInputs, and will directly read from the source.\nNetwork isolation is not supported for parametrized data sources.\n</div>", "metadata": {}}, {"id": "a9bae76c", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from sagemaker.processing import ProcessingInput, ProcessingOutput\nfrom sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n\ndata_sources = []", "outputs": []}, {"id": "e122e5c6", "cell_type": "markdown", "source": "## Input - S3 Source: data.csv", "metadata": {}}, {"id": "2ac81ef4", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "data_sources.append(ProcessingInput(\n    source=\"s3://mlschools-data-jerryb/penguins/data/data.csv\", # You can override this to point to other dataset on S3\n    destination=\"/opt/ml/processing/data.csv\",\n    input_name=\"data.csv\",\n    s3_data_type=\"S3Prefix\",\n    s3_input_mode=\"File\",\n    s3_data_distribution_type=\"FullyReplicated\"\n))", "outputs": []}, {"id": "baa087b8", "cell_type": "markdown", "source": "## Output: S3 settings\n\n<div class=\"alert alert-info\"> \ud83d\udca1 <strong> Configurable Settings </strong>\n\n1. <b>bucket</b>: you can configure the S3 bucket where Canvas will save the output. The default bucket from \nthe SageMaker notebook session is used. \n2. <b>flow_export_id</b>: A randomly generated export id. The export id must be unique to ensure the results do not \nconflict with other flow exports \n3. <b>s3_ouput_prefix</b>:  you can configure the directory name in your bucket where your data will be saved.\n</div>", "metadata": {}}, {"id": "aeef5ed5", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import time\nimport uuid\nimport boto3\nimport sagemaker\n\n# Sagemaker session\nsess = sagemaker.Session(default_bucket='mlschools-data-jerryb')\n\nregion = boto3.Session().region_name\n\n# You can configure this with your own bucket name, e.g.\n# bucket = \"my-bucket\"\nbucket = sess.default_bucket()\nprint(f\"Canvas export storage bucket: {bucket}\")\n\n# unique flow export ID\nflow_export_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\nflow_export_name = f\"flow-{flow_export_id}\"", "outputs": []}, {"id": "cf55a868", "cell_type": "markdown", "source": "Below are the inputs required by the SageMaker Python SDK to launch a processing job.", "metadata": {}}, {"id": "a6038dea", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Output name is auto-generated from the select node's ID + output name from the flow file.\noutput_name = \"c38bfbfd-a35f-4492-978f-f3ab0dfd9d18.default\"\n\ns3_output_prefix = f\"export-{flow_export_name}/output\"\ns3_output_base_path = f\"s3://{bucket}/{s3_output_prefix}\"\nprint(f\"Processing output base path: {s3_output_base_path}\\nThe final output location will contain additional subdirectories.\")\n\nprocessing_job_output = ProcessingOutput(\n    output_name=output_name,\n    source=\"/opt/ml/processing/output\",\n    destination=s3_output_base_path,\n    s3_upload_mode=\"EndOfJob\"\n)", "outputs": []}, {"id": "15f261f8", "cell_type": "markdown", "source": "## Specify Flow S3 Location\n\nTo use the data flow as an input to the processing job, specify its location in S3.", "metadata": {}}, {"id": "8e84e2f8", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import os\nimport json\n\nflow_s3_uri = \"s3://mlschools-data-jerryb/penguins/output_1707528456/penguins.flow\"\n\nprint(f\"Data flow is located at {flow_s3_uri}\")", "outputs": []}, {"id": "5d0a3f4d", "cell_type": "markdown", "source": "The data flow is also provided to the Processing Job as an input source which we configure below.", "metadata": {}}, {"id": "6a7b7488", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "## Input - Flow: penguins.flow\nflow_input = ProcessingInput(\n    source=flow_s3_uri,\n    destination=\"/opt/ml/processing/flow\",\n    input_name=\"flow\",\n    s3_data_type=\"S3Prefix\",\n    s3_input_mode=\"File\",\n    s3_data_distribution_type=\"FullyReplicated\"\n)", "outputs": []}, {"id": "c11106e9", "cell_type": "markdown", "source": "# Run Processing Job \n## Job Configurations\n\n<div class=\"alert alert-info\"> \ud83d\udca1 <strong> Configurable Settings </strong>\n\nYou can configure the following settings for Processing Jobs. If you change any configurations you will \nneed to re-execute this and all cells below it by selecting the Run menu above and click \n<b>Run Selected Cells and All Below</b>\n\n1. IAM role for executing the processing job. \n2. A unique name of the processing job. Give a unique name every time you re-execute processing jobs\n3. Canvas Container URL.\n4. Instance count, instance type and storage volume size in GB.\n5. Content type for each output. Canvas supports CSV as default and Parquet.\n6. Network Isolation settings\n7. KMS key to encrypt output data\n</div>", "metadata": {}}, {"id": "0171eaca", "cell_type": "code", "metadata": {"tags": ["parameters"]}, "execution_count": null, "source": "from sagemaker import image_uris\n\n# IAM role for executing the processing job.\niam_role = sagemaker.get_execution_role()\n\n# Unique processing job name. Give a unique name every time you re-execute processing jobs.\nprocessing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n\n# Canvas Container URL.\ncontainer_uri = \"599662218115.dkr.ecr.us-east-2.amazonaws.com/sagemaker-data-wrangler-container:4.x\"\n# Pinned Canvas Container URL.\ncontainer_uri_pinned = \"599662218115.dkr.ecr.us-east-2.amazonaws.com/sagemaker-data-wrangler-container:4.2.0\"\n\n# Processing Job Instance count and instance type.\ninstance_count = 2\ninstance_type = \"ml.m5.4xlarge\"\n\n# Size in GB of the EBS volume to use for storing data during processing.\nvolume_size_in_gb = 30\n\n\n# Content type for each output. Canvas supports CSV as default and Parquet.\noutput_content_type = \"CSV\"\n\n# Delimiter to use for the output if the output content type is CSV. Uncomment to set.\n# delimiter = \",\"\n\n# Compression to use for the output. Uncomment to set.\n# compression = \"gzip\"\n\n# Configuration for partitioning the output. Uncomment to set.\n# \"num_partition\" sets the number of partitions/files written in the output.\n# \"partition_by\" sets the column names to partition the output by.\n# partition_config = {\n#     \"num_partitions\": 1,\n#     \"partition_by\": [\"column_name_1\", \"column_name_2\"],\n# }\n\n# Network Isolation mode; default is off.\nenable_network_isolation = False\n\n# List of tags to be passed to the processing job.\nuser_tags = None\n\n# Output configuration used as processing job container arguments. Only applies when writing to S3.\n# Uncomment to set additional configurations.\noutput_config = {\n    output_name: {\n        \"content_type\": output_content_type,\n        # \"delimiter\": delimiter,\n        # \"compression\": compression,\n        # \"partition_config\": partition_config,\n    }\n}\n\n# Refit configuration determines whether Canvas refits the trainable parameters on the entire dataset. \n# When True, the processing job relearns the parameters and outputs a new flow file.\n# You can specify the name of the output flow file under 'output_flow'.\n# Note: There are length constraints on the container arguments (max 256 characters).\nuse_refit = False\nrefit_trained_params = {\n    \"refit\": use_refit,\n    \"output_flow\": f\"data-wrangler-flow-processing-{flow_export_id}.flow\"\n}\n\n# KMS key for per object encryption; default is None.\nkms_key = None\n\n# Inference parameters determine whether Canvas generates an inference artifact at the end of the job run.\n# When set, the processing job generates an inference artifact and uploads it to S3 under the S3 prefix of `flow_s3_uri`.\n# You can specify the name of the output artifact with 'inference_artifact_name'.\nuse_inference_params = False\ninference_artifact_name = f\"data-wrangler-flow-processing-{flow_export_id}.tar.gz\"\ninference_params = {\n    \"inference_artifact_name\": inference_artifact_name,\n    \"output_node_id\": output_name.split(\".\")[0]\n}", "outputs": []}, {"id": "9b30607d", "cell_type": "markdown", "source": "### Job arguments", "metadata": {}}, {"id": "0751009f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "job_arguments = [f\"--output-config '{json.dumps(output_config)}'\"]\nif use_refit:\n    job_arguments.append(f\"--refit-trained-params '{json.dumps(refit_trained_params)}'\")\nif use_inference_params:\n    job_arguments.append(f\"--inference-params '{json.dumps(inference_params)}'\")", "outputs": []}, {"id": "f72934bc", "cell_type": "markdown", "source": "### (Optional) Configure Spark Cluster Driver Memory", "metadata": {}}, {"id": "1e5f5bad", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# The Spark memory configuration. Change to specify the driver and executor memory in MB for the Spark cluster during processing.\ndriver_memory_in_mb = 55742\nexecutor_memory_in_mb = 55742\n\nconfig = json.dumps({\n    \"Classification\": \"spark-defaults\",\n    \"Properties\": {\n        \"spark.driver.memory\": f\"{driver_memory_in_mb}m\",\n        \"spark.executor.memory\": f\"{executor_memory_in_mb}m\"\n    }\n})\n\n# Provides the spark config file to processing job and set the cluster driver memory. Uncomment to set.\n# config_file = f\"config-{flow_export_id}.json\"\n# with open(config_file, \"w\") as f:\n#     f.write(config)\n\n# config_s3_path = f\"spark_configuration/{processing_job_name}/configuration.json\"\n# config_s3_uri = f\"s3://{bucket}/{config_s3_path}\"\n# s3_client.upload_file(config_file, bucket, config_s3_path, ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n# print(f\"Spark Config file uploaded to {config_s3_uri}\")\n# os.remove(config_file)\n\n# data_sources.append(ProcessingInput(\n#     source=config_s3_uri,\n#     destination=\"/opt/ml/processing/input/conf\",\n#     input_name=\"spark-config\",\n#     s3_data_type=\"S3Prefix\",\n#     s3_input_mode=\"File\",\n#     s3_data_distribution_type=\"FullyReplicated\"\n# ))", "outputs": []}, {"id": "75b81fd5", "cell_type": "markdown", "source": "## Create Processing Job\n\nTo launch a Processing Job, you will use the SageMaker Python SDK to create a Processor function.", "metadata": {}}, {"id": "d116942b", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Setup processing job network configuration\nfrom sagemaker.network import NetworkConfig\n\nnetwork_config = NetworkConfig(\n    enable_network_isolation=enable_network_isolation,\n    security_group_ids=None,\n    subnets=None\n)", "outputs": []}, {"id": "5b4b125e", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from sagemaker.processing import Processor\n\nprocessor = Processor(\n    role=iam_role,\n    image_uri=container_uri,\n    instance_count=instance_count,\n    instance_type=instance_type,\n    volume_size_in_gb=volume_size_in_gb,\n    network_config=network_config,\n    sagemaker_session=sess,\n    output_kms_key=kms_key,\n    tags=user_tags\n)\n\n# Start Job\nprocessor.run(\n    inputs=[flow_input] + data_sources, \n    outputs=[processing_job_output],\n    arguments=job_arguments,\n    wait=False,\n    logs=False,\n    job_name=processing_job_name\n)", "outputs": []}, {"id": "eb93c748", "cell_type": "markdown", "source": "## Job Status & S3 Output Location\n\nBelow you wait for processing job to finish. If it finishes successfully, the raw parameters used by the \nProcessing Job will be printed.\n\nTo prevent data of different processing jobs and different output nodes from being overwritten or combined, \nCanvas uses the name of the processing job and the name of the output to write the output.", "metadata": {}}, {"id": "401322d0", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "s3_job_results_path = f\"{s3_output_base_path}/{processing_job_name}/{output_name.replace('.', '/')}\"\nprint(f\"Job results are saved to S3 path: {s3_job_results_path}\")\n\njob_result = sess.wait_for_processing_job(processing_job_name)\njob_result", "outputs": []}, {"id": "4cb751fa", "cell_type": "markdown", "source": "## (Optional)Next Steps\n\nNow that data is available on S3 you can use other SageMaker components that take S3 URIs as input such as \nSageMaker Training, Built-in Algorithms, etc. Similarly you can load the dataset into a Pandas dataframe \nin this notebook for further inspection and work. The examples below show how to do both of these steps.", "metadata": {}}, {"id": "6bfa48c9", "cell_type": "markdown", "source": "By default optional steps do not run automatically, set `run_optional_steps` to True if you want to \nexecute optional steps", "metadata": {}}, {"id": "b30a8ff9", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "run_optional_steps = False", "outputs": []}, {"id": "7dcf87b0", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# This will stop the below cells from executing if \"Run All Cells\" was used on the notebook.\nif not run_optional_steps:\n    raise SystemExit(\"Stop here. Do not automatically execute optional steps.\")", "outputs": []}, {"id": "4354b730", "cell_type": "markdown", "source": "### (Optional) Load Processed Data into Pandas\n\nWe use the [AWS SDK for pandas library](https://github.com/awslabs/aws-sdk-pandas) to load the exported \ndataset into a Pandas data frame for a preview of first 10000 rows.\n\nTo turn on automated visualizations and data insights for your pandas data frame, import the sagemaker_datawrangler library.", "metadata": {}}, {"id": "b073ee82", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "!pip install -q awswrangler pandas\nimport awswrangler as wr\n\n# Import sagemaker_datawrangler to show visualizations and automated data\n# quality insights, and export code to prepare data in a pandas data frame.\ntry:\n    import sagemaker_datawrangler\nexcept ImportError:\n    print(\"sagemaker_datawrangler is not imported. Change your kernel to the Data Science 3.0 Kernel Image and try again.\")\n    pass", "outputs": []}, {"id": "6254913f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "chunksize = 10000\n\nif output_content_type.upper() == \"CSV\":\n    dfs = wr.s3.read_csv(s3_job_results_path, chunksize=chunksize)\nelif output_content_type.upper() == \"PARQUET\":\n    dfs = wr.s3.read_parquet(s3_job_results_path, chunked=chunksize)\nelse:\n    print(f\"Unexpected output content type {output_content_type}\") \n\ndf = next(dfs)\ndf", "outputs": []}, {"id": "7416e7dd", "cell_type": "markdown", "source": "## (Optional)Train a model with SageMaker\nNow that the data has been processed, you may want to train a model using the data. The following shows an \nexample of doing so using a popular algorithm - XGBoost. For more information on algorithms available in \nSageMaker, see [Getting Started with SageMaker Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html). \nIt is important to note that the following XGBoost objective ['binary', 'regression', 'multiclass'] \nhyperparameters, or content_type may not be suitable for the output data, and will require changes to \ntrain a proper model. Furthermore, for CSV training, the algorithm assumes that the target \nvariable is in the first column. For more information on SageMaker XGBoost, \nsee https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html.\n\n\n### Set Training Data path\nWe set the training input data path from the output of the Canvas processing job..", "metadata": {}}, {"id": "9fa4f56d", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "s3_training_input_path = s3_job_results_path\nprint(f\"training input data path: {s3_training_input_path}\")", "outputs": []}, {"id": "3a0f6a21", "cell_type": "markdown", "source": "### Configure the algorithm and training job\n\nThe Training Job hyperparameters are set. For more information on XGBoost Hyperparameters, \nsee https://xgboost.readthedocs.io/en/latest/parameter.html.", "metadata": {}}, {"id": "0ac8603d", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "region = boto3.Session().region_name\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\nhyperparameters = {\n    \"max_depth\":\"5\",\n    \"objective\": \"reg:squarederror\",\n    \"num_round\": \"10\",\n}\ntrain_content_type = (\n    \"application/x-parquet\" if output_content_type.upper() == \"PARQUET\"\n    else \"text/csv\"\n)\ntrain_input = sagemaker.inputs.TrainingInput(\n    s3_data=s3_training_input_path,\n    content_type=train_content_type,\n)", "outputs": []}, {"id": "e22384d2", "cell_type": "markdown", "source": "### Start the Training Job\n\nThe TrainingJob configurations are set using the SageMaker Python SDK Estimator, and which is fit using \nthe training data from the Processing Job that was run earlier.", "metadata": {}}, {"id": "852a9393", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "estimator = sagemaker.estimator.Estimator(\n    container,\n    iam_role,\n    hyperparameters=hyperparameters,\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n)\nestimator.fit({\"train\": train_input})", "outputs": []}, {"id": "01bf76cb", "cell_type": "markdown", "source": "Now that you have a trained model there are a number of different things you can do. \nFor more details on training with SageMaker, please see \nhttps://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html.", "metadata": {}}]}